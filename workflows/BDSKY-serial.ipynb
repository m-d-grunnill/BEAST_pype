{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee348d7-8a73-4f0e-95ec-1418ebfa2161",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Birth-Death Skyline (BDSKY) serial workflow\n",
    "\n",
    "\n",
    "This Workflow Notebook is for running BDSKY serial models in BEAST 2. **The template BEAST 2 xml (template_xml) or ready_to_go_xml provided must be for BDSKY serial!**\n",
    "\n",
    "\n",
    "```\n",
    "Parameters\n",
    "-------------\n",
    "overall_save_dir: str\n",
    "    Path to where you are saving all the runs of this pipeline.\n",
    "\n",
    "specific_run_save_dir: str, default a timestamp of format 'YYYY-MM-DD_hour-min-sec'\n",
    "    Subdirectory of overall_save_dir you wish to save the outputs from this pipeline iine. \n",
    "    If None, 'None' or an emply string a timestamp of format 'YYYY-MM-DD_hour-min-sec' is used instead.\n",
    "\n",
    "cache_name str, default 'cache'\n",
    "    Name to use for cache directory. Saved within overall_save_dir/specific_run_save_dir but deleted the end of this\n",
    "    workflow notebook.\n",
    "\n",
    "initial_tree_path: str, opional\n",
    "    Path to initial tree to use in generating a BEAST 2 xml. Should be .nwk file (Newick format).\n",
    "    If provided phases 2i and 2ii are skiped.\n",
    "    If a distance tree is used set initial_tree_type to 'Distance'.\n",
    "    If a temporal tree is used set initial_tree_type to 'Temporal'.\n",
    "\n",
    "use_initial_tree:  bool, default True\n",
    "    If False an initial tree will not be generated skipping Phases 2i and 2ii. As such, in phase 4 BEAST 2 generate its own\n",
    "    initial tree.\n",
    "\n",
    "initial_tree_type: str (either 'Distance' or 'Temporal') or None, default 'Temporal'\n",
    "    Intial tree type to use.\n",
    "    If 'Distance' and initial_tree_path is not provided the IQtree tree from Phase-2i-IQTree.ipynb is used for the\n",
    "    initial tree and phase 2ii is skipped.\n",
    "    if 'Temporal' and initial_tree_path is not provided the TreeTime tree from Phase-2ii-TreeTime-and-Down-Sampling.ipynb\n",
    "    is used for the initial tree.\n",
    "\n",
    "ready_to_go_xml: str, optional\n",
    "    Path to a BEAST 2 xml that you wish to run unaltered. If provided phases 2i, 2ii and 3 are skipped.\n",
    "\n",
    "fasta_file: str\n",
    "    Path to fasta file containing sequences to use in generating a BEAST 2 xml.\n",
    "\n",
    "partition: str\n",
    "    The name of partition to use when launching slurm jobs via `sbatch` in phases 2i and 4.\n",
    "\n",
    "metadata_path: str\n",
    "    Path to csv or tsv containing metadata pertaining to fasta_file.\n",
    "\n",
    "sample_id_field: str, default 'strain'\n",
    "    Name of field in metadata_db containing ids corresponding to those used in fasta_file.\n",
    "\n",
    "collection_date_field: str, default 'date'\n",
    "    Name of field in metadata_db containing collection dates of sequences. Should be format YYYY-MM-DD.\n",
    "\n",
    "root_strain_names: list of strings, optional\n",
    "     IDs of sequences used to root 'Temporal' initial_tree removed from fasta file and initial tree file used to generate\n",
    "    the BEAST 2 xml.\n",
    "\n",
    "down_sample_to: int, optional\n",
    "    If provided the fasta file and initial tree file used to generate the BEAST 2 xml is downsampled to this amount.\n",
    "    If downsampling occurs the following are saved in  '{overall_save_dir}/{specific_run_save_dir}/' and used in generating\n",
    "    a BEAST 2 xml in phase 4:\n",
    "        down_sampled_time.nwk: A downsampled temporal tree.\n",
    "        down_sampled_sequences.fasta: Fasta file containing downsamplec sequences.\n",
    "        down_sampled_metadata.csv: the down sampled metadata.\n",
    "\n",
    "template_xml_path:\n",
    "    Path to template BEAST 2 xml used to generate the BEAST 2 xml.\n",
    "\n",
    "rt_dimensions: int, default 6\n",
    "    Number of Rt dimensions (time periods over which Rt is estimated).\n",
    "\n",
    "origin_start_addition float\n",
    "    This + initial temporal tree height is used as starting value of origin.\n",
    "    We recommend using an estimate of infection period for the pathogen being studied. **Value should be in years.**\n",
    "    Origin prio will be unform:\n",
    "        Lower value: time in years from oldest to youngest sequence in fasta_file\n",
    "        Start value: origin_start_addition + initial temporal tree height\n",
    "        Upper value:  initial temporal tree height + origin_upper_addition.\n",
    "\n",
    "origin_upper_addition: float/int\n",
    "    This + initial temporal tree height is used as upper value of origin prior. **Value should be in years.**\n",
    "    Origin prio will be unform:\n",
    "        Lower value: time in years from oldest to youngest sequence in fasta_file\n",
    "        Start value: origin_start_addition + initial temporal tree height\n",
    "        Upper value:  initial temporal tree height + origin_upper_addition.\n",
    "\n",
    "origin_prior: dict {'lower': float, 'upper': float, 'start': float}, optional\n",
    "       Details of the origin prior. assumed to be uniformly distributed.\n",
    "\n",
    "log_file_basename: str, optional\n",
    "    If provided .tree, .log and .state files from running BEAST 2 will have this name prefixed by 'run-{number}-',\n",
    "    number being that of the chain.\n",
    "\n",
    "chain_length: int\n",
    "    Number of chains to use for BEAST runs.\n",
    "\n",
    "trace_log_every: int\n",
    "    How often to save a log file during BEAST runs.\n",
    "\n",
    "tree_log_every: int\n",
    "    How often to save a tree file during BEAST runs.\n",
    "\n",
    "screen_log_every: int\n",
    "    How often to output to screen during BEAST runs.\n",
    "\n",
    "store_state_every: int \n",
    "    How often to store MCMC state during BEAST runs.\n",
    "\n",
    "number_of_chains: int\n",
    "    Number of chains to use (repeated runs to do) when running BEAST.\n",
    "\n",
    "seeds: list of ints, optional\n",
    "    Seeds to use when running BEAST. If not provided seeds are drawn using numpy's randint function\n",
    "    (interges 1 to 1,000,000).\n",
    "\n",
    "beast_ram: str\n",
    "    RAM to use for each run of beast see sbatch documentation.\n",
    "\n",
    "beast_threads: int\n",
    "    Threads/CPUs to use for each run of beast see https://www.beast2.org/2021/03/31/command-line-options.html.\n",
    "\n",
    "burnin_percentage: int\n",
    "    Perecentage burnin to use.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375522b9e88db977",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Needed in this notebook\n",
    "overall_save_dir = '../example_runs_of_BSDKY'\n",
    "specific_run_save_dir=None\n",
    "cache_name='cache'\n",
    "initial_tree_path = None\n",
    "use_initial_tree = True\n",
    "initial_tree_type = 'Temporal'\n",
    "ready_to_go_xml = None\n",
    "\n",
    "\n",
    "# Used in Phase 2i\n",
    "fasta_file= '../example_data/COVID-19_BA.2.86/sequences.fasta'\n",
    "partition=None\n",
    "\n",
    "# Used in Phase 2ii\n",
    "metadata_path = '../example_data/COVID-19_BA.2.86/metadata.tsv'\n",
    "sample_id_field='strain'\n",
    "collection_date_field='date'\n",
    "root_strain_names=None\n",
    "down_sample_to=None\n",
    "\n",
    "# Used in Phase 3:\n",
    "#fasta_file = 'example_data/COVID-19_BSDKY/sequences.fasta_file'  Defined above\n",
    "#metadata_path = None Defined above\n",
    "template_xml_path = '../template_beast_xmls/BDSKY_serial_COVID-19_template.xml'\n",
    "rt_dimensions=6\n",
    "\n",
    "# collection_date_field='date' Defined above\n",
    "origin_start_addition = 10 / 365.25\n",
    "origin_upper_addition = 2\n",
    "origin_prior = None\n",
    "log_file_basename='BDSKY_serial'\n",
    "chain_length = int(5e5)\n",
    "trace_log_every = int(5e2)\n",
    "tree_log_every = int(5e2)\n",
    "screen_log_every = int(5e2)\n",
    "store_state_every = int(5e2)\n",
    "\n",
    "# Used in Phase 4:\n",
    "number_of_chains = 4\n",
    "#partition='NMLResearch' Defined above\n",
    "seeds = None\n",
    "beast_ram = \"32G\"\n",
    "beast_threads=6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50fb2a4-ae8c-42db-a96f-e73675bd415a",
   "metadata": {},
   "source": [
    "## Import libraries and define functions:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd379001fecb8061",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from beast_pype.workflow import check_file_for_phrase\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import papermill as pm\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "from numpy.random import randint \n",
    "import shutil\n",
    "from papermill.iorw import load_notebook_node, write_ipynb\n",
    "from papermill.parameterize import parameterize_notebook\n",
    "import importlib.resources as importlib_resources\n",
    "\n",
    "def cell_variables_to_dict(offset=0):\n",
    "    \"\"\"\n",
    "    Convert the variables of a cell to a dictionary, using names as keys.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    offset: int, default 0\n",
    "        How many cells ago:\n",
    "            0: for current cell.\n",
    "            1: for the previous called cell.\n",
    "            2: for the cell before that.\n",
    "            ......\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary : dict {name_of_variable: value}\n",
    "\n",
    "    References\n",
    "    ----------------\n",
    "    https://stackoverflow.com/questions/46824287/print-all-variables-defined-in-one-jupyter-cell\n",
    "    \n",
    "    Note\n",
    "    -------\n",
    "    Not sure why bet this needs to be defined within the notebook and not imported.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import io # for some searson this has to be called within the function.\n",
    "    from contextlib import redirect_stdout # for some searson this has to be called within the function.\n",
    "    ipy = get_ipython()\n",
    "    out = io.StringIO()\n",
    "\n",
    "    with redirect_stdout(out):\n",
    "        ipy.run_line_magic(\"history\", str(ipy.execution_count - offset))\n",
    "\n",
    "    #process each line...\n",
    "    x = out.getvalue().replace(\" \", \"\").split(\"\\n\")\n",
    "    x = [a.split(\"=\")[0] for a in x if \"=\" in a] #all of the variables in the cell\n",
    "    g = globals()\n",
    "    dictionary = {k:g[k] for k in x if k in g}\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88ac24-3421-42d3-8e48-07e3cb3d88b7",
   "metadata": {},
   "source": [
    "### Check parameters are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8683e15a08892",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ready_to_go_xml is not None or not use_initial_tree: # None of the parameters below are needed if a ready_to_go_xml is provided.\n",
    "    if initial_tree_type not in ['Temporal', 'Distance', None]:\n",
    "        raise ValueError('initial_tree_type must be either \"Temporal\" or \"Distance\" or None.')\n",
    "\n",
    "    if initial_tree_path is not None and initial_tree_type is None:\n",
    "        raise ValueError('initial_tree_type must be specified if initial_tree_path is given.')\n",
    "\n",
    "    if origin_start_addition is not None:\n",
    "        if initial_tree_type != 'Temporal':\n",
    "            raise ValueError('origin_start_addition is reliant on the initial_tree_type being \"Temporal\".')\n",
    "        if origin_upper_addition is None:\n",
    "            raise ValueError('origin_start_addition is reliant on origin_upper_addition being given as well.')\n",
    "\n",
    "    if origin_upper_addition is not None:\n",
    "        if initial_tree_type != 'Temporal':\n",
    "            raise ValueError('origin_upper_addition is reliant on the initial_tree_type being \"Temporal\".')\n",
    "        if origin_upper_addition is None:\n",
    "            raise ValueError('origin_upper_addition is reliant on origin_start_addition being given as well.')\n",
    "\n",
    "    if initial_tree_type in ['Distance', None]:\n",
    "        if origin_prior is None:\n",
    "            raise ValueError('If initial_tree_type is \"Distance\" or an an initial tree is not being used, an origin_prior must be given.')\n",
    "        if origin_start_addition is not None:\n",
    "            raise ValueError('If initial_tree_type is \"Distance\" or an an initial tree is not being used, an origin_start_addition should NOT be given.')\n",
    "        if origin_upper_addition is not None:\n",
    "            raise ValueError('If initial_tree_type is \"Distance\" or an an initial tree is not being used, an origin_upper_addition should NOT be given.')\n",
    "\n",
    "    if down_sample_to is not None and (initial_tree_path is not None or initial_tree_type == 'Distance'):\n",
    "        raise ValueError(\"Currently beast_pype's down_sampling method is tied to its Tree Time tree building.\" +\n",
    "                         \"Therefore, to use this down sampling method an initial_tree_path should not be given and an initial_tree_type should be set to 'Temporal'.\")\n",
    "\n",
    "if partition is None:\n",
    "    raise ValueError('The name of partition to use when launching slurm jobs via `sbatch` in phases 2i and 4, must be given.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2df4abae881b4",
   "metadata": {},
   "source": [
    "Record parameters in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4bf9301ae8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = cell_variables_to_dict(offset=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a9be5-25c6-43e4-8a43-e677a2ee19f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating Folders and Subfolders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d4f34-9b4b-432b-b471-0f15b2803dab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(overall_save_dir):\n",
    "    os.makedirs(overall_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720f264-a51c-4f55-b671-f7cdf227a4af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Create a folder of today's date to save into within save_dir and reassign save_dir to that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b12632-5558-45f4-aaca-1431d410491d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if specific_run_save_dir is None or specific_run_save_dir in ['', 'None']:\n",
    "    now = datetime.now()\n",
    "    specific_run_save_dir = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "save_dir = overall_save_dir +'/'+ specific_run_save_dir\n",
    "cache_dir = f'{save_dir}/{cache_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9f69d-4b8d-4e22-8d52-7332a2f40e1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If save_dir and and cache_dir do not exist create them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cda57c804317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in [save_dir, cache_dir]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76650944-127e-4e06-82d1-b412047faaf8",
   "metadata": {},
   "source": [
    "Start recording pipeline_run_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8bd74-c16e-4903-8265-dcd9476a06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run_info = {'parameters': parameters}\n",
    "\n",
    "with open(save_dir +'/pipeline_run_info.json', 'w') as fp:\n",
    "    json.dump(pipeline_run_info, fp, sort_keys=True, indent=4)\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb10b7079780bc",
   "metadata": {},
   "source": [
    "### Placing Common Parameters in a Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904ad069c255f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_params = {\n",
    "    'save_dir' : save_dir,\n",
    "    'cache_dir' : cache_dir\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9197b63a1b94b12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Creating a record for runtimes\n",
    "\n",
    "This record list of dirtionaries will be turned into a pandas dataframe and saved as a csv at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb3f93570c1555",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_records = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c7107c7a63d66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Set path to workflow modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9769dc07371196",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_modules = importlib_resources.path('beast_pype', 'workflow_modules')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1a451cd296447",
   "metadata": {},
   "source": [
    "## Phase 2: Data Pre-Processing\n",
    "### Phase 2i: Building an IQ Tree tree.\n",
    "#### Placing Phase 2i Parameters in a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff432a6c2458f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ready_to_go_xml is None and \\\n",
    "    use_initial_tree and \\\n",
    "        (initial_tree_path is None and initial_tree_type is not None):\n",
    "    phase_2i_start = perf_counter()\n",
    "    phase_2i_params = {**common_params,\n",
    "                       **{val_name: eval(val_name) for val_name in ['fasta_file' , 'partition']}\n",
    "                     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67764fd8d3b3915",
   "metadata": {},
   "source": [
    "#### Running Phase 2i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1b379f5b32eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Phase-2i-IQTree.ipynb\n",
    "if ready_to_go_xml is None and \\\n",
    "    use_initial_tree and \\\n",
    "        (initial_tree_path is None and initial_tree_type is not None):\n",
    "    phase_2i_log = pm.execute_notebook(input_path=f'{workflow_modules}/Phase-2i-IQTree.ipynb',\n",
    "                                      output_path=save_dir + '/Phase-2i-IQTree.ipynb',\n",
    "                                      parameters=phase_2i_params,\n",
    "                                      progress_bar=True,\n",
    "                                      nest_asyncio=True\n",
    "                                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4dcab4174a53e4",
   "metadata": {},
   "source": [
    "### Wait for IQtree to be Built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16982f5d12320ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ready_to_go_xml is None and \\\n",
    "    use_initial_tree and \\\n",
    "        (initial_tree_path is None and initial_tree_type is not None):\n",
    "    out_file =  f'{save_dir}/iqtree.out'\n",
    "    check_file_for_phrase(out_file)\n",
    "\n",
    "    runtime_records.append({\n",
    "        'Phase': 'Phase-2i-IQTree.ipynb',\n",
    "        'Sample': None,\n",
    "        'Chain': None,\n",
    "        'Runtime': perf_counter() - phase_2i_start\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ab22e07bc1d69",
   "metadata": {},
   "source": [
    "### Phase 2ii: Building an TreeTime tree and Downsampling.\n",
    "#### Placing Phase 2ii Parameters in a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc06dc53c93c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ready_to_go_xml is None and \\\n",
    "    use_initial_tree and \\\n",
    "        (initial_tree_path is None and initial_tree_type =='Temporal'):\n",
    "    phase_2ii_start = perf_counter()\n",
    "    phase_2ii_params = {**common_params,\n",
    "                        **{val_name: eval(val_name) for val_name in ['fasta_file',\n",
    "                                                                     'metadata_path',\n",
    "                                                                     'sample_id_field',\n",
    "                                                                     'collection_date_field',\n",
    "                                                                     'down_sample_to',\n",
    "                                                                     'root_strain_names']}\n",
    "                     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee70b1e226ef72",
   "metadata": {},
   "source": [
    "#### Running Phase 2ii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323efa6440ea2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Phase-2ii-TreeTime-and-Down-Sampling.ipynb\n",
    "if ready_to_go_xml is None and \\\n",
    "    use_initial_tree and \\\n",
    "        (initial_tree_path is None and initial_tree_type =='Temporal'):\n",
    "    phase_2ii_log = pm.execute_notebook(input_path=f'{workflow_modules}/Phase-2ii-TreeTime-and-Down-Sampling.ipynb',\n",
    "                                      output_path=save_dir + '/Phase-2ii-TreeTime-and-Down-Sampling.ipynb',\n",
    "                                      parameters=phase_2ii_params,\n",
    "                                      progress_bar=True,\n",
    "                                      nest_asyncio=True\n",
    "                                     )\n",
    "\n",
    "    runtime_records.append({\n",
    "        'Phase': 'Phase-2ii-TreeTime-and-Down-Sampling.ipynb',\n",
    "        'Sample': None,\n",
    "        'Chain': None,\n",
    "        'Runtime': perf_counter() - phase_2i_start\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3f672959e2da4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Phase 3 Generating BEAST xmls\n",
    "\n",
    "### Placing Phase 3 Parameters in a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1974fdd7c329fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ready_to_go_xml is None:\n",
    "    phase_3_start = perf_counter()\n",
    "\n",
    "    phase_3_params = {\n",
    "        **common_params,\n",
    "        **{val_name: eval(val_name) for val_name in [\n",
    "            'template_xml_path',\n",
    "            'use_initial_tree',\n",
    "            'initial_tree_path',\n",
    "            'rt_dimensions',\n",
    "            'collection_date_field',\n",
    "            'origin_start_addition',\n",
    "            'origin_upper_addition',\n",
    "            'log_file_basename',\n",
    "            'origin_prior',\n",
    "            'chain_length',\n",
    "            'trace_log_every',\n",
    "            'tree_log_every',\n",
    "            'screen_log_every',\n",
    "            'store_state_every']}}\n",
    "\n",
    "    if down_sample_to is None:\n",
    "        phase_3_params['fasta_file'] = fasta_file\n",
    "        phase_3_params['metadata_path'] = metadata_path\n",
    "    else:\n",
    "        phase_3_params['fasta_file'] = f'{save_dir}/down_sampled_sequences.fasta_file'\n",
    "        phase_3_params['metadata_path'] =  f'{save_dir}/down_sampled_metadata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38109bbeca2516f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Running Phase 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7145ca6f91b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Phase-3-Gen-BDSKY-xml.ipynb\n",
    "if ready_to_go_xml is None:\n",
    "    phase_3_log = pm.execute_notebook(input_path=f'{workflow_modules}/Phase-3-Gen-BDSKY-xml.ipynb',\n",
    "                                      output_path=save_dir + '/Phase-3-Gen-BDSKY-xml.ipynb',\n",
    "                                      parameters=phase_3_params,\n",
    "                                      progress_bar=True,\n",
    "                                      nest_asyncio=True)\n",
    "    runtime_records.append({\n",
    "        'Phase': 'Phase-3-Gen-BDSKY-xml.ipynb',\n",
    "        'Sample': None,\n",
    "        'Chain': None,\n",
    "        'Runtime': perf_counter() - phase_3_start\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c940e-a5fa-454f-a652-128f4ad19d6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Phase 4 Running BEAST\n",
    "\n",
    "BEASTs random number seed can select the same seed for multiple runs if they are launched close together in time (such as proromatically). Therefore lets use numpy to generate our seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ea52e-c776-461c-86b0-241587cd8e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if seeds is None:\n",
    "    number_of_seeds=number_of_chains\n",
    "    seeds = randint(low=1, high=int(1e6), size=number_of_seeds).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25a3dc-cbb6-42ad-8329-dc005b373933",
   "metadata": {},
   "source": [
    "Record seeds in pipeline_run_info json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86526e0-af8d-4a8b-9a7e-617ac61ab7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_dir + \"/pipeline_run_info.json\", \"r\") as file:\n",
    "    data = file.read()\n",
    "file.close()\n",
    "pipeline_run_info = json.loads(data)\n",
    "pipeline_run_info['seeds'] = seeds\n",
    "with open(save_dir +'/pipeline_run_info.json', 'w') as fp:\n",
    "    json.dump(pipeline_run_info, fp, sort_keys=True, indent=4)\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640ddf03679e6fb",
   "metadata": {},
   "source": [
    "### If ready_to_go_xml was provided save a copy for use as f'{save_dir}/beast.xml'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea9deb8c1ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ready_to_go_xml is not None:\n",
    "    shutil.copy(ready_to_go_xml, f'{save_dir}/beast.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1651ff3f101a85f",
   "metadata": {},
   "source": [
    "### Placing Phase 4 Parameters in a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee687e7ca93f2d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_4_params = {**common_params,\n",
    "                  'number_of_chains': number_of_chains,\n",
    "                  'seeds':seeds,\n",
    "                  'partition': partition,\n",
    "                  'beast_threads':beast_threads,\n",
    "                  'beast_ram':beast_ram}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7d043c49bfebc",
   "metadata": {},
   "source": [
    "### Running Phase 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7c179ecf1f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Phase5-Running-BEAST.ipynb\n",
    "phase_4_log = pm.execute_notebook(input_path=f'{workflow_modules }/Phase-4-Running-BEAST.ipynb',\n",
    "                                  output_path=save_dir + '/Phase-4-Running-BEAST.ipynb',\n",
    "                                  parameters=phase_4_params,\n",
    "                                  progress_bar=True,\n",
    "                                  nest_asyncio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc28550baefae7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Add Slurm Job IDs and Names to pipeline_run_info.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd050169b8ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_dir + \"/pipeline_run_info.json\", \"r\") as file:\n",
    "    data = file.read()\n",
    "file.close()\n",
    "pipeline_run_info = json.loads(data)\n",
    "with open(cache_dir +'/slurm_job_ids.txt', 'r') as fp:\n",
    "    entries = fp.read().splitlines() \n",
    "fp.close()\n",
    "pipeline_run_info['slurm job IDs'] = entries\n",
    "with open(save_dir +'/pipeline_run_info.json', 'w') as fp:\n",
    "    json.dump(pipeline_run_info, fp, sort_keys=True, indent=4)\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "600fca04-3d24-45c1-bbb5-ac496a5e05a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Phase 5: Diagnosing Outputs and Generate Report\n",
    "\n",
    "Curently this has to be performed manually. That being said, the code cell below will parameterize a copy of the notebook ready to run. See below for location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f01c5b4d87487",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phase_5_params = {'save_dir': None,\n",
    "                  'report_template': str(importlib_resources.path('beast_pype', 'report_templates') / 'BDSKY-Serial-Report.ipynb'),\n",
    "                  'add_unreported_fields': True,\n",
    "                  'metadata_path':os.path.abspath(metadata_path),\n",
    "                  } # in this case the metadata_path needs to be absolute.\n",
    "\n",
    "phase_5_notebook = load_notebook_node(f'{workflow_modules}/Phase-5-Diagnosing_Outputs_and_Generate_Report.ipynb')\n",
    "phase_5_notebook = parameterize_notebook(phase_5_notebook, phase_5_params)\n",
    "write_ipynb(phase_5_notebook, f'{save_dir}/Phase-5-Diagnosing_Outputs_and_Generate_Report.ipynb')\n",
    "print(f'Phase 5 notebook is ready for manual use at: \\n{save_dir}/Phase-5-Diagnosing_Outputs_and_Generate_Report.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73996f7cb93da2a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Recording Runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5327106e3a7f45",
   "metadata": {},
   "source": [
    "Converting to pandas DataFrame and saving as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ce3afca8ba34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_df = pd.DataFrame.from_records(runtime_records)\n",
    "runtime_df.sort_values(['Phase', 'Sample', 'Runtime'], inplace=True)\n",
    "runtime_df.to_csv(save_dir + \"/runtimes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac9202-fc54-476e-8a6f-4e20a0663218",
   "metadata": {},
   "source": [
    "### Delete Cache direcory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be66ce-465f-4538-b5eb-83ff4324d344",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(cache_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beast_pype",
   "language": "python",
   "name": "beast_pype"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
